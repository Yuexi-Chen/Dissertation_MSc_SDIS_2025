
This repository contains all code and data used in my MSc dissertation project.

## 📁 Repository Structure

```
project-root/
│
├── codes/                 # All code snippets generated by LLMs
├── config/                # Configuration files for tasks, models, languages, and prompt completeness
├── prompt_templates/      # Prompt templates used to assemble full prompts before sending to LLMs
├── results/               # Raw outputs including runtime logs and static analysis (NDJSON format)
├── results_analysis/      # Scripts and merged data for statistical analysis and figure/table generation
├── scripts/               # Pipeline scripts for running the full experimental process
├── src/                   # Core modules of the experimental process
└── test_cases/            # Test cases for each task (in JSON format)

```

## ⚙️ Requirements

- Python 3.13.0
- Required libraries listed in `requirements.txt`
- SonarQube (local server required for static analysis)
- Environment token required for SonarQube (set as environment variable)
- API key required for LLM APIs

## 🚀 How to Run

1. **Install dependencies**  
```bash
pip install -r requirements.txt
```

2. **Set experimental environment (Windows)**  
- Set LLMs' API keys and different tasks/completeness/languages/models you want to evaluate in `config/config.json`
- Set SonarCube token in `scripts/set_env.ps1`

3. **Run a full pipeline for one scenario**  
- Step 1. Code Generation:
```bash
python scripts/verify_generator.py
```

- Step 2. Testing:
```bash
python scripts/verify_test.py
```

- Step 3. Static Analyze:
```bash
# Set SonarCube token as environment variable
# It will automatically run `verify_analyzer.py`
powershell -ExecutionPolicy Bypass -File .\scripts\set_env.ps1

# Run hallucination analyzer manually
python scripts/verify_h_analyzer.py
```

**Output**  
- `results/execution/`: Runtime metadata
- `results/generation/`: Generation metadata
- `results/hallucination/`: Hallucination metadata
- `results/static_analysis/`: Static analysis results from SonarCube
- `results/test_results/`: Test results for each test case

## 📊 Metrics

The following metrics are automatically calculated by the evaluation module (`results_analysis/metrics.py` and `results_analysis/table.py`) based on the consolidated output file `merged_results.ndjson`:

1. Functional Correctness (FC)
2. Execution Quality Score (EQS)
3. Hallucination Rate (HR)
4. Readability (R)
5. Security (S)
6. Robustness (RB)
7. Composite Quality Score (CQS)

### Command-Line Tutorial for `table.py`

This script generates pivot tables from evaluation data.


#### Customizing Table Structure

Control the rows, columns, and values displayed in the table:

```bash
cd results_analysis/

python table.py --index language --columns model --value functional_correctness
```
*   `--index`: Specifies the row index(es). Use `metrics` to show metrics as rows.
*   `--columns`: Specifies the column index. Use `metrics` to show metrics as columns.
*   `--value`: Specifies the metric to display in the table cells (e.g., `readability`, `robustness`, `cqs`). Use short names like `fc`, `r`, `rb`, `m`, `s`, `hr` for convenience.

#### Other Options

*   `--output_file`: Customize the output location and filename.
*   `--column_order`: Specify a custom order for columns.
*   `--no_rank`, `--no_sort`: Disable ranking and sorting by rank.

Combine arguments as needed to generate your desired table. For detailed help, run:
```bash
python table.py --help
```


## 📦 Notes

- All outputs are in `.ndjson` format. The file `merged_results.ndjson` is the unified output from all ndjson files, used for result analysis.



